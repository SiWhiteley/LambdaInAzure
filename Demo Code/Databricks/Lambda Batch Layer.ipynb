{"cells":[{"cell_type":"code","source":["#I'm using credentials created previously using databricks secrets\n#More info here: https://docs.azuredatabricks.net/user-guide/secrets/secrets.html\nClientId = dbutils.secrets.get(scope = \"Lambda\", key = \"ClientId\")\nClientSecret = dbutils.secrets.get(scope = \"Lambda\", key = \"ClientSecret\")\nTenantName = dbutils.secrets.get(scope = \"Lambda\", key = \"TenantName\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#I now need to configure ADLS to use my credentials\nspark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\nspark.conf.set(\"dfs.adls.oauth2.client.id\", ClientId)\nspark.conf.set(\"dfs.adls.oauth2.credential\", ClientSecret)\nspark.conf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/\"+TenantName+\"/oauth2/token\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#I'm using the current date/time to work out the active file, so lets exclude it\nimport datetime\nts = datetime.datetime.utcnow().strftime('%Y/%m/%d/%H_')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["## Infer the schema from a known sample file\nsampDF = sqlContext.read \\\n  .format(\"com.databricks.spark.csv\") \\\n  .option(\"header\",\"true\") \\\n  .option(\"inferSchema\",\"true\") \\\n  .load(\"adl://[ADLS ACCOUNT].azuredatalakestore.net/[SAMPLE FILE LOCATION]\")\n\n## Create DataFrame over all files, using inferred schema. Append FilePath\nspeedDF = sqlContext.read \\\n  .format(\"com.databricks.spark.csv\") \\\n  .schema(sampDF.schema) \\\n  .option(\"header\", \"true\") \\\n  .load(\"adl://[ADLS ACCOUNT].azuredatalakestore.net/[ROOT FOLDER FOR DATASET]/*/*/*/*.csv\") \\\n  .selectExpr('*','input_file_name() as filename')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Use Filename Column to selectively filter files\nfilterDF = speedDF.filter(~speedDF.filename.like(ts))\n\nfilterDF.createOrReplaceTempView(\"Sales\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Now lets actually do some data processing. This is a very small example, it would normally be our full ETL process!\naggDF = spark.sql(\"SELECT productname ProductName, sum(salesamount) TotalSales FROM Sales GROUP BY productname\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# We'll be writing the final results to sqldw via blob storage, so we need some more credentials\nUserName = dbutils.secrets.get(scope = \"Lambda\", key = \"username\")\nPassword = dbutils.secrets.get(scope = \"Lambda\", key = \"password\")\nBlobKey = dbutils.secrets.get(scope = \"Lambda\", key = \"blobkey\")\n\n# Set up the Blob storage account access key in the notebook session conf.\nspark.conf.set(\n  \"fs.azure.account.key.[BLOB ACCOUNT NAME].blob.core.windows.net\",\n  BlobKey)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Finally, we push the data down to SQLDW via polybase\naggDF.write \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", \"jdbc:sqlserver://[SQLDW SERVER NAME].database.windows.net;database=[DATABASE NAME]\") \\\n  .option(\"user\", UserName)\\\n  .option(\"password\", Password) \\\n  .option(\"tempDir\", \"wasbs://[BLOB CONTAINER]@[BLOB ACCOUNT].blob.core.windows.net/Files\") \\\n  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n  .option(\"dbtable\", \"dbo.BatchLayer\") \\\n  .mode(\"overwrite\") \\\n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"Lambda Batch Layer","notebookId":4255681438186523},"nbformat":4,"nbformat_minor":0}
